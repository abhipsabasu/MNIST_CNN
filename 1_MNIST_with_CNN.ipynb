{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. MNIST with CNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A2g4-_eNmQZ",
        "colab_type": "code",
        "outputId": "d1d56884-0823-42c5-91ed-6ae2f1fbe3b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "dataset_path = 'gdrive/My Drive/Deep Learning/MNIST with CNN/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Am72CjV-_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = dataset_path + \"train.csv\"\n",
        "test_path = dataset_path + \"test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K2c6_YsXOEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnCOatf9Xo6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = train[\"label\"]\n",
        "\n",
        "# Drop 'label' column\n",
        "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
        "\n",
        "X_train = X_train / 255.0\n",
        "test = test / 255.0\n",
        "\n",
        "X_train = X_train.values.reshape(-1,28,28,1)\n",
        "test = test.values.reshape(-1,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A0m8BhmYTsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = to_categorical(Y_train, num_classes = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8f_c3FLYdrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "random_seed = 2\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1VU1tY8YVaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (28,28,1)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = \"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTER0aCoYjwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0,\n",
        "                 amsgrad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nbZadcdb_eW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzYZatkqcEyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pkLSNr-cJQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAlZNj8bcWhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YC7fiWqcZoW",
        "colab_type": "code",
        "outputId": "062d48d5-00bd-49c8-d0bb-50002625f8bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1300
        }
      },
      "source": [
        "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
        "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
        "                              , callbacks=[learning_rate_reduction])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " - 9s - loss: 0.4763 - acc: 0.8457 - val_loss: 0.0715 - val_acc: 0.9786\n",
            "Epoch 2/30\n",
            " - 8s - loss: 0.1435 - acc: 0.9572 - val_loss: 0.0528 - val_acc: 0.9831\n",
            "Epoch 3/30\n",
            " - 8s - loss: 0.1132 - acc: 0.9661 - val_loss: 0.0485 - val_acc: 0.9845\n",
            "Epoch 4/30\n",
            " - 9s - loss: 0.0887 - acc: 0.9731 - val_loss: 0.0311 - val_acc: 0.9907\n",
            "Epoch 5/30\n",
            " - 8s - loss: 0.0748 - acc: 0.9780 - val_loss: 0.0409 - val_acc: 0.9881\n",
            "Epoch 6/30\n",
            " - 8s - loss: 0.0697 - acc: 0.9785 - val_loss: 0.0253 - val_acc: 0.9924\n",
            "Epoch 7/30\n",
            " - 8s - loss: 0.0650 - acc: 0.9803 - val_loss: 0.0319 - val_acc: 0.9907\n",
            "Epoch 8/30\n",
            " - 8s - loss: 0.0576 - acc: 0.9833 - val_loss: 0.0302 - val_acc: 0.9921\n",
            "Epoch 9/30\n",
            " - 8s - loss: 0.0566 - acc: 0.9826 - val_loss: 0.0307 - val_acc: 0.9902\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 10/30\n",
            " - 8s - loss: 0.0452 - acc: 0.9863 - val_loss: 0.0182 - val_acc: 0.9936\n",
            "Epoch 11/30\n",
            " - 8s - loss: 0.0410 - acc: 0.9872 - val_loss: 0.0168 - val_acc: 0.9957\n",
            "Epoch 12/30\n",
            " - 8s - loss: 0.0408 - acc: 0.9875 - val_loss: 0.0171 - val_acc: 0.9950\n",
            "Epoch 13/30\n",
            " - 8s - loss: 0.0376 - acc: 0.9885 - val_loss: 0.0174 - val_acc: 0.9943\n",
            "Epoch 14/30\n",
            " - 9s - loss: 0.0362 - acc: 0.9898 - val_loss: 0.0171 - val_acc: 0.9948\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 15/30\n",
            " - 8s - loss: 0.0320 - acc: 0.9904 - val_loss: 0.0156 - val_acc: 0.9960\n",
            "Epoch 16/30\n",
            " - 8s - loss: 0.0288 - acc: 0.9908 - val_loss: 0.0180 - val_acc: 0.9948\n",
            "Epoch 17/30\n",
            " - 8s - loss: 0.0273 - acc: 0.9918 - val_loss: 0.0154 - val_acc: 0.9952\n",
            "Epoch 18/30\n",
            " - 8s - loss: 0.0274 - acc: 0.9913 - val_loss: 0.0143 - val_acc: 0.9957\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 19/30\n",
            " - 8s - loss: 0.0276 - acc: 0.9916 - val_loss: 0.0140 - val_acc: 0.9952\n",
            "Epoch 20/30\n",
            " - 8s - loss: 0.0269 - acc: 0.9922 - val_loss: 0.0146 - val_acc: 0.9960\n",
            "Epoch 21/30\n",
            " - 8s - loss: 0.0233 - acc: 0.9932 - val_loss: 0.0149 - val_acc: 0.9952\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 22/30\n",
            " - 8s - loss: 0.0245 - acc: 0.9924 - val_loss: 0.0154 - val_acc: 0.9955\n",
            "Epoch 23/30\n",
            " - 8s - loss: 0.0219 - acc: 0.9935 - val_loss: 0.0135 - val_acc: 0.9957\n",
            "Epoch 24/30\n",
            " - 9s - loss: 0.0228 - acc: 0.9932 - val_loss: 0.0159 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 25/30\n",
            " - 9s - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0156 - val_acc: 0.9957\n",
            "Epoch 26/30\n",
            " - 8s - loss: 0.0217 - acc: 0.9933 - val_loss: 0.0142 - val_acc: 0.9955\n",
            "Epoch 27/30\n",
            " - 8s - loss: 0.0218 - acc: 0.9936 - val_loss: 0.0144 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 28/30\n",
            " - 8s - loss: 0.0219 - acc: 0.9936 - val_loss: 0.0151 - val_acc: 0.9955\n",
            "Epoch 29/30\n",
            " - 8s - loss: 0.0216 - acc: 0.9934 - val_loss: 0.0147 - val_acc: 0.9960\n",
            "Epoch 30/30\n",
            " - 8s - loss: 0.0203 - acc: 0.9938 - val_loss: 0.0145 - val_acc: 0.9960\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfKfp5Y8cstN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(test)#predict for the test data\n",
        "\n",
        "# take the index of highest probability \n",
        "results = np.argmax(results,axis = 1)\n",
        "#Convert the array to a pandas series\n",
        "results = pd.Series(results,name=\"Label\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jVpS6JWoLVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
        "\n",
        "submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsMnVN6KoQvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}